{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khanh22mse23057/bigdata/blob/main/%5BMSE_HCM%5D%5BBigData%5DKhanhPNP_Final_term_exam.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "</br>SUBJECT: BIGDATA\n",
        "</br>CLASS : MSE11#HCM\n",
        "</br>MSHV : 22MSE23057\n",
        "</br>SUTEDENT's NAME : PHAM NGUYEN PHU KHANH\n",
        "</br>INSTRUCTOR: MAI HOANG BAO AN"
      ],
      "metadata": {
        "id": "lIU5miFnvnvF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  I. Theoretical (30 marks)\n",
        "Below are the theoretical session, which requires you to write down in thw cell \"Text\" in the notebook:\n",
        "\n",
        "\n",
        "1. Could you list out the main challenges of concurrency?\n",
        "2. Could you describe shortly about MapReduce? Please provide an example of MapReduce.\n",
        "3. Provide a high level comparison of Apache Hadoop and Apache Spark.\n",
        "4. What are the advantages of Apache Spark?\n",
        "5. Provide a comparison of RDD and DataFrame in Spark.  "
      ],
      "metadata": {
        "id": "byA90itmbBOR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Could you list out the main challenges of concurrency?"
      ],
      "metadata": {
        "id": "JwU0QcnKcg9_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Concurrency is the capacity of a system to execute multiple tasks simultaneously or in parallel, and it presents a number of challenges. Among the primary challenges of concurrency are:\n",
        "\n",
        "\n",
        "\n",
        "1.  **Race Conditions**: occur when multiple threads or processes attempt to access and modify shared resources concurrently, resulting in unpredictable and incorrect results.\n",
        "\n",
        "2.   **The deadlock**: occurs when two or more processes cannot proceed because each is waiting for the other to relinquish a resource, resulting in a circular dependency.\n",
        "\n",
        "3. **Resource Management**: In concurrent systems, it is essential to effectively manage shared resources to prevent contention and ensure equitable access.\n",
        "\n",
        "4. **Synchronization Overhead**: Adding synchronization mechanisms, such as locks, to coordinate concurrent access can introduce latency and potentially affect performance.\n",
        "\n",
        "5. **Thread Safety**: entails ensuring that data and code are accurately shared and accessed by multiple threads without causing unexpected behavior. This can be a difficult but essential task.\n",
        "\n",
        "6. **Load Balancing**: For optimal performance, evenly distributing workloads across threads or processes can be challenging.\n",
        "\n",
        "7. **Parallelism Scaling**: It can be challenging to scale a concurrent system to take advantage of multiple cores or processors without experiencing diminishing returns.\n",
        "\n",
        "8. **Debugging and Testing**: Identifying and resolving bugs in concurrent code can be more difficult than in sequential programs, and verifying all possible execution paths can be difficult.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gDHeY4ppcljR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Could you describe shortly about MapReduce? Please provide an example of MapReduce."
      ],
      "metadata": {
        "id": "7DRjLEjueHNG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "MapReduce is a programming model and processing technique for distributed data processing. It was popularized by Google and allows processing large datasets in parallel across a cluster of computers.\n",
        "\n",
        "In MapReduce, data processing is divided into two main phases:\n",
        "\n",
        "Map Phase: The input data is split into smaller chunks, and each chunk is processed independently by mapper tasks. These mappers extract relevant information and generate key-value pairs as intermediate outputs.\n",
        "\n",
        "Reduce Phase: The intermediate key-value pairs generated by the mappers are shuffled and sorted based on their keys and then processed by reducer tasks. The reducers aggregate, combine, or analyze the data to produce the final output.\n",
        "\n",
        "Example of MapReduce:\n",
        "\n",
        "Let's consider a simple word count example using MapReduce. Suppose we have a large text document, and we want to count the occurrences of each word in the document.\n",
        "\n",
        "1. Map Phase: The input text is divided into smaller segments. Each mapper takes a segment and processes it independently. It scans the text, splits it into words, and emits key-value pairs with the word as the key and a count of 1 as the value. For example:\n",
        "\n",
        "Input: \"Hello world, this is a hello world example.\""
      ],
      "metadata": {
        "id": "XcDqQtSuef3u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(\"Hello\", 1)\n",
        "(\"world,\", 1)\n",
        "(\"this\", 1)\n",
        "(\"is\", 1)\n",
        "(\"a\", 1)\n",
        "(\"hello\", 1)\n",
        "(\"world\", 1)\n",
        "(\"example.\", 1)"
      ],
      "metadata": {
        "id": "h_q690ZRq5kV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24515764-69b4-43ff-ce35-692f122c2583"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('example.', 1)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Reduce Phase: The intermediate key-value pairs are grouped by keys, and each reducer processes a group of values for a specific key. The reducer sums up the counts for each word and emits the final output. For example:\n",
        "\n",
        "Reducer Input:"
      ],
      "metadata": {
        "id": "JKrpACy3eyb6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(\"Hello\", [1])\n",
        "(\"world\", [1, 1])\n",
        "(\"this\", [1])\n",
        "(\"is\", [1])\n",
        "(\"a\", [1])\n",
        "(\"example.\", [1])"
      ],
      "metadata": {
        "id": "OtMcLgl_e3-4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe586d37-ec81-4f51-fbf9-6e567c270b87"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('example.', [1])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reducer Output:"
      ],
      "metadata": {
        "id": "ThaR8N6He-pM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(\"Hello\", 1)\n",
        "(\"world\", 2)\n",
        "(\"this\", 1)\n",
        "(\"is\", 1)\n",
        "(\"a\", 1)\n",
        "(\"example.\", 1)"
      ],
      "metadata": {
        "id": "7LbglYhXe_Gg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1158a123-3174-4bbd-c2f3-b74d832ee97b"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('example.', 1)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Provide a high level comparison of Apache Hadoop and Apache Spark."
      ],
      "metadata": {
        "id": "G93AREc2ePgI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apache Hadoop and Apache Spark are both distributed data processing frameworks, but they have some differences in terms of design and capabilities."
      ],
      "metadata": {
        "id": "VKXeiXCxfBWn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1. **Data Processing Model:**\n",
        "</br>**Hadoop**: Primarily based on the MapReduce model, which is suitable for batch processing of large datasets.\n",
        "</br>**Spark**: Provides a more versatile and flexible processing model, supporting batch processing, interactive queries, real-time stream processing, and machine learning.\n",
        "\n",
        "2. **Performance**\n",
        "</br>**Hadoop**: Suffers from performance overhead due to the disk-based storage of intermediate data between Map and Reduce phases.\n",
        "</br>**Spark**: In-memory processing capability enables faster data access and iterative computations, making it generally faster than Hadoop's MapReduce for many use cases.\n",
        "\n",
        "3. **Ease of Use**\n",
        "</br>**Hadoop**: Requires more low-level coding with MapReduce, making it complex for developers.\n",
        "</br>**Spark**: Offers higher-level APIs in Java, Scala, Python, and R, which are easier to use and develop applications.\n",
        "\n",
        "4. **Fault Tolerance:**\n",
        "</br>**Hadoop**: Provides fault tolerance through data replication, which can lead to higher storage overhead.\n",
        "</br>**Spark**: Achieves fault tolerance through lineage information, allowing it to reconstruct lost data by recomputing it, which reduces storage overhead.\n",
        "\n",
        "\n",
        "5. **Caching and Data Sharing:**\n",
        "</br>**Hadoop**: Lacks efficient caching mechanisms and data sharing across multiple computations.\n",
        "</br>**Spark**: Provides RDD (Resilient Distributed Dataset) abstraction, allowing in-memory caching and data sharing across multiple Spark operations, which improves performance.\n",
        "\n",
        "\n",
        "6. **Use Cases:**\n",
        "</br>**Hadoop**: Better suited for large-scale batch processing jobs, especially when data locality is essential.\n",
        "</br>**Spark**: Well-suited for iterative algorithms, interactive data analysis, real-time streaming, and machine learning tasks."
      ],
      "metadata": {
        "id": "85Fd9UaZfElX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. What are the advantages of Apache Spark?"
      ],
      "metadata": {
        "id": "BN0MqpbceRMb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Advantages of Apache Spark**:\n",
        "\n",
        "1. **Speed**: Spark's in-memory processing and optimized execution plans result in significantly faster data processing compared to traditional disk-based systems like Hadoop MapReduce.\n",
        "\n",
        "2. **Ease of Use**: Spark offers high-level APIs in various programming languages, making it easier for developers to write complex distributed data processing applications with less code.\n",
        "\n",
        "3. **Versatility**: Spark supports various data processing tasks, including batch processing, real-time stream processing, interactive queries, and machine learning, all in the same unified framework.\n",
        "\n",
        "4. **Fault Tolerance**: Spark provides fault tolerance through lineage information, reducing the need for excessive data replication and minimizing storage overhead.\n",
        "\n",
        "5. **In-Memory Caching**: Spark allows users to cache intermediate data in memory, enabling faster access and iterative processing, which is especially useful for machine learning algorithms.\n",
        "\n",
        "6. **Integration**: Spark integrates well with other big data technologies, such as Hadoop Distributed File System (HDFS), Apache Hive, Apache HBase, and more, making it a part of existing data ecosystems.\n",
        "\n",
        "7. **Community and Ecosystem**: Spark has a thriving open-source community, which leads to continuous development, improvement, and a rich ecosystem of libraries and extensions."
      ],
      "metadata": {
        "id": "yS49_XqSgb5g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Provide a comparison of RDD and DataFrame in Spark.\n",
        "\n",
        "Comparison of RDD(Resilient Distributed Dataset) and DataFrame in Spark:"
      ],
      "metadata": {
        "id": "mheusVLSeRZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apache Spark is an open-source distributed processing platform designed to handle big data workloads.\n",
        "</br>RDD is a read-only collection of different types of objects, while Dataframe is a distributed collection of a dataset.\n",
        "\n",
        "1.\n",
        "</br>RDD is useful for processing computations on large clusters within memory in a resilient manner, increasing task efficiency. It is useful for unstructured data, low-level transformations, and schema-based data manipulation.\n",
        "</br>Dataframes, on the other hand, are a fixed distributed data collection that allows higher-level abstractions. They are useful for structured or semi-structured data, storing one-dimensional or multidimensional data matrices in tabular form, and high-level processing in datasets.\n",
        "\n",
        "2.\n",
        "</br>RDD can process structured and unstructured data, but it does not provide the schema of added data.\n",
        "</br>Dataframes can process structured and semi-structured data only because they are like a relational database and can manage the schema.\n",
        "\n",
        "3.\n",
        "</br>RDD supports OOP through compile-time type safety, while Dataframe does not. RDD's immutability ensures consistent calculations, while Dataframes cannot.\n",
        "\n",
        "4.\n",
        "</br>RDD has use cases such as handling data with no predefined structure, calculations right after actions, projects based on Java, Scala, R, and Python, specifying a schema, and high-level abstraction and low-level transformation.\n",
        "</br>Dataframes, on the other hand, can handle data from specific sources like JSON, MySQL, CSV, and can be used for high-level abstraction and low-level transformations."
      ],
      "metadata": {
        "id": "9N98CZT8hM5t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise"
      ],
      "metadata": {
        "id": "iz224F28i-Dn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup Environment"
      ],
      "metadata": {
        "id": "9oVfI9izjWei"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop3.2\"\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jDBXd3wxjbLZ",
        "outputId": "099e6b67-9d52-4c1f-b2a8-2a2e6d04f48b"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "\r0% [Waiting for headers] [Connecting to security.ubuntu.com (185.125.190.39)] [\r                                                                               \rHit:2 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "\r0% [Waiting for headers] [Waiting for headers] [Waiting for headers] [Connected\r                                                                               \rHit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "\r0% [Waiting for headers] [Waiting for headers] [Connected to ppa.launchpadconte\r                                                                               \rHit:4 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:5 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:6 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:7 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkConf, SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# sc = SparkContext(conf=SparkConf())\n",
        "# spark = SparkSession(sparkContext=sc)"
      ],
      "metadata": {
        "id": "_k8SOjEnlPXs"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Input Data Preparation"
      ],
      "metadata": {
        "id": "L8fJ0vVfjeV-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/khanh22mse23057/bigdata/raw/main//final_exam/appl_stock.csv\n",
        "!wget https://github.com/khanh22mse23057/bigdata/raw/main//final_exam/customer_churn.csv\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IC3iGNUUjnap",
        "outputId": "eadfaa3e-6474-4009-c8bb-24ec3ba65e27"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-07-22 09:03:44--  https://github.com/khanh22mse23057/bigdata/raw/main//final_exam/appl_stock.csv\n",
            "Resolving github.com (github.com)... 140.82.112.3\n",
            "Connecting to github.com (github.com)|140.82.112.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/khanh22mse23057/bigdata/main/final_exam/appl_stock.csv [following]\n",
            "--2023-07-22 09:03:44--  https://raw.githubusercontent.com/khanh22mse23057/bigdata/main/final_exam/appl_stock.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 143130 (140K) [text/plain]\n",
            "Saving to: ‘appl_stock.csv.1’\n",
            "\n",
            "appl_stock.csv.1    100%[===================>] 139.78K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2023-07-22 09:03:45 (6.30 MB/s) - ‘appl_stock.csv.1’ saved [143130/143130]\n",
            "\n",
            "--2023-07-22 09:03:45--  https://github.com/khanh22mse23057/bigdata/raw/main//final_exam/customer_churn.csv\n",
            "Resolving github.com (github.com)... 140.82.112.3\n",
            "Connecting to github.com (github.com)|140.82.112.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/khanh22mse23057/bigdata/main/final_exam/customer_churn.csv [following]\n",
            "--2023-07-22 09:03:45--  https://raw.githubusercontent.com/khanh22mse23057/bigdata/main/final_exam/customer_churn.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 115479 (113K) [text/plain]\n",
            "Saving to: ‘customer_churn.csv.1’\n",
            "\n",
            "customer_churn.csv. 100%[===================>] 112.77K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2023-07-22 09:03:45 (5.15 MB/s) - ‘customer_churn.csv.1’ saved [115479/115479]\n",
            "\n",
            "appl_stock.csv\t    customer_churn.csv.1       spark-3.1.1-bin-hadoop3.2.tgz\n",
            "appl_stock.csv.1    sample_data\t\t       spark-3.1.1-bin-hadoop3.2.tgz.1\n",
            "customer_churn.csv  spark-3.1.1-bin-hadoop3.2  spark-3.1.1-bin-hadoop3.2.tgz.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Requirements"
      ],
      "metadata": {
        "id": "mDGjuraWktHj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### II. (30 marks)\n",
        "You are given a file `appl_stock.csv`, please carry out the following tasks:\n",
        "\n",
        "1. Read this file by PySpark. Print out the schema.\n",
        "2. Create columns of `day of month`, `hour`, `day of year`, `month` from the column `Date` of the data.\n",
        "3. Using `groupby` and `year()` function to compute the average closing price per year."
      ],
      "metadata": {
        "id": "S9_563ulpsh9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.Read this file by PySpark. Print out the schema."
      ],
      "metadata": {
        "id": "XCf84aiRn0Oj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _read_file(_fPath, header=True, inferSchema=True):\n",
        "\n",
        "    # Step 1: Read the CSV file and print the schema\n",
        "    df = spark.read.csv(_fPath, header=True, inferSchema=True)\n",
        "\n",
        "    # Print the schema\n",
        "    df.printSchema()\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "2_IOI4Y7lXaf"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "original_df = _read_file(\"./appl_stock.csv\")\n",
        "original_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqG3Jem2kqSQ",
        "outputId": "dfc6c531-bc4d-40f5-d6c9-b56423783333"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Date: string (nullable = true)\n",
            " |-- Open: double (nullable = true)\n",
            " |-- High: double (nullable = true)\n",
            " |-- Low: double (nullable = true)\n",
            " |-- Close: double (nullable = true)\n",
            " |-- Volume: integer (nullable = true)\n",
            " |-- Adj Close: double (nullable = true)\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Row(Date='2010-01-04', Open=213.429998, High=214.499996, Low=212.38000099999996, Close=214.009998, Volume=123432400, Adj Close=27.727039)"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.Create columns of day of month, hour, day of year, month from the column Date of the data."
      ],
      "metadata": {
        "id": "IeKLo56An3Oi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import dayofmonth, hour, dayofyear, month\n",
        "from pyspark.sql import DataFrame\n",
        "\n",
        "def add_date_columns(df: DataFrame, date_column_name: str) -> DataFrame:\n",
        "    df_with_day_of_month = df.withColumn(\"DayOfMonth\", dayofmonth(df[date_column_name]))\n",
        "    df_with_hour = df_with_day_of_month.withColumn(\"Hour\", hour(df_with_day_of_month[date_column_name]))\n",
        "    df_with_day_of_year = df_with_hour.withColumn(\"DayOfYear\", dayofyear(df_with_hour[date_column_name]))\n",
        "    df_with_month = df_with_day_of_year.withColumn(\"Month\", month(df_with_day_of_year[date_column_name]))\n",
        "\n",
        "    return df_with_month"
      ],
      "metadata": {
        "id": "JJGOMOyamRBR"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming you have a DataFrame named 'original_df' with a column 'Date'\n",
        "new_df = add_date_columns(original_df, \"Date\")\n",
        "# Assuming you have a DataFrame named 'new_df'\n",
        "new_df.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yw6qiBFEmUC4",
        "outputId": "a84a274d-cdf4-4812-f4e4-4341440f880e"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+----------+------------------+------------------+---------+------------------+----------+----+---------+-----+\n",
            "|      Date|      Open|      High|               Low|             Close|   Volume|         Adj Close|DayOfMonth|Hour|DayOfYear|Month|\n",
            "+----------+----------+----------+------------------+------------------+---------+------------------+----------+----+---------+-----+\n",
            "|2010-01-04|213.429998|214.499996|212.38000099999996|        214.009998|123432400|         27.727039|         4|   0|        4|    1|\n",
            "|2010-01-05|214.599998|215.589994|        213.249994|        214.379993|150476200|27.774976000000002|         5|   0|        5|    1|\n",
            "|2010-01-06|214.379993|    215.23|        210.750004|        210.969995|138040000|27.333178000000004|         6|   0|        6|    1|\n",
            "|2010-01-07|    211.75|212.000006|        209.050005|            210.58|119282800|          27.28265|         7|   0|        7|    1|\n",
            "|2010-01-08|210.299994|212.000006|209.06000500000002|211.98000499999998|111902700|         27.464034|         8|   0|        8|    1|\n",
            "+----------+----------+----------+------------------+------------------+---------+------------------+----------+----+---------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.Using groupby and year() function to compute the average closing price per year."
      ],
      "metadata": {
        "id": "X90n0hsQoBV4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import year, avg\n",
        "\n",
        "\n",
        "# Group by year and compute the average closing price for each year\n",
        "average_closing_per_year_df = new_df.groupBy(year(\"Date\").alias(\"Year\")).agg(avg(\"Close\").alias(\"Average Closing Price\")).orderBy(\"Year\")\n",
        "\n",
        "# Show the result\n",
        "average_closing_per_year_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYZse4SAm_Bj",
        "outputId": "193757ab-9c0c-4b94-c590-83782e7d212c"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+---------------------+\n",
            "|Year|Average Closing Price|\n",
            "+----+---------------------+\n",
            "|2010|    259.8424600000002|\n",
            "|2011|   364.00432532142867|\n",
            "|2012|    576.0497195640002|\n",
            "|2013|    472.6348802857143|\n",
            "|2014|    295.4023416507935|\n",
            "|2015|   120.03999980555547|\n",
            "|2016|   104.60400786904763|\n",
            "+----+---------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### III. (40 marks)\n",
        "You are given a data `customer_churn.csv`, which describes the churn status in clients of a marletting agency. As a data scientist, you are required to create a machine learning model **in Spark** that will help predict which customers will churn (stop buying their service). A short description of the data is as follow:\n",
        "```\n",
        "Name : Name of the latest contact at Company\n",
        "Age: Customer Age\n",
        "Total_Purchase: Total Ads Purchased\n",
        "Account_Manager: Binary 0=No manager, 1= Account manager assigned\n",
        "Years: Totaly Years as a customer\n",
        "Num_sites: Number of websites that use the service.\n",
        "Onboard_date: Date that the name of the latest contact was onboarded\n",
        "Location: Client HQ Address\n",
        "Company: Name of Client Company\n",
        "```\n",
        "\n",
        "1. Read, print the schema and check out the data to set the first sight of the data.\n",
        "2. Format the data according to `VectorAssembler`, which is supported in MLlib of PySpark.\n",
        "3. Split the data into train/test data, and then fit train data to the logistic regression model.\n",
        "4. Evaluate the results and compute the AUC."
      ],
      "metadata": {
        "id": "brQ8gRaUshXy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.Read, print the schema and check out the data to set the first sight of the data."
      ],
      "metadata": {
        "id": "VYhc13j5obZd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "customer_churn_df = _read_file(\"./customer_churn.csv\")\n",
        "customer_churn_df.head()"
      ],
      "metadata": {
        "id": "fN4Zb88PtzCL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56a7be3d-c321-43ba-c9c1-ed2164e83da2"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Names: string (nullable = true)\n",
            " |-- Age: double (nullable = true)\n",
            " |-- Total_Purchase: double (nullable = true)\n",
            " |-- Account_Manager: integer (nullable = true)\n",
            " |-- Years: double (nullable = true)\n",
            " |-- Num_Sites: double (nullable = true)\n",
            " |-- Onboard_date: string (nullable = true)\n",
            " |-- Location: string (nullable = true)\n",
            " |-- Company: string (nullable = true)\n",
            " |-- Churn: integer (nullable = true)\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Row(Names='Cameron Williams', Age=42.0, Total_Purchase=11066.8, Account_Manager=0, Years=7.22, Num_Sites=8.0, Onboard_date='2013-08-30 07:00:40', Location='10265 Elizabeth Mission Barkerburgh, AK 89518', Company='Harvey LLC', Churn=1)"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "customer_churn_df.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4jMmIHOuorZj",
        "outputId": "ae905db0-fcc2-4609-8564-6eb63b98585d"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+----+--------------+---------------+-----+---------+-------------------+--------------------+--------------------+-----+\n",
            "|           Names| Age|Total_Purchase|Account_Manager|Years|Num_Sites|       Onboard_date|            Location|             Company|Churn|\n",
            "+----------------+----+--------------+---------------+-----+---------+-------------------+--------------------+--------------------+-----+\n",
            "|Cameron Williams|42.0|       11066.8|              0| 7.22|      8.0|2013-08-30 07:00:40|10265 Elizabeth M...|          Harvey LLC|    1|\n",
            "|   Kevin Mueller|41.0|      11916.22|              0|  6.5|     11.0|2013-08-13 00:38:46|6157 Frank Garden...|          Wilson PLC|    1|\n",
            "|     Eric Lozano|38.0|      12884.75|              0| 6.67|     12.0|2016-06-29 06:20:07|1331 Keith Court ...|Miller, Johnson a...|    1|\n",
            "|   Phillip White|42.0|       8010.76|              0| 6.71|     10.0|2014-04-22 12:43:12|13120 Daniel Moun...|           Smith Inc|    1|\n",
            "|  Cynthia Norton|37.0|       9191.58|              0| 5.56|      9.0|2016-01-19 15:31:15|765 Tricia Row Ka...|          Love-Jones|    1|\n",
            "+----------------+----+--------------+---------------+-----+---------+-------------------+--------------------+--------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.Format the data according to VectorAssembler, which is supported in MLlib of PySpark."
      ],
      "metadata": {
        "id": "Reukex9Mo04Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "def format_df_by_cols(input_columns):\n",
        "\n",
        "  # Create a SparkSession (if not already created)\n",
        "  spark = SparkSession.builder.appName(\"VectorAssemblerExam\").getOrCreate()\n",
        "\n",
        "  # Create the VectorAssembler instance\n",
        "  vector_assembler = VectorAssembler(inputCols=input_columns, outputCol=\"features\")\n",
        "\n",
        "  # Transform the DataFrame to include the 'features' column\n",
        "  formatted_data_df = vector_assembler.transform(customer_churn_df)\n",
        "\n",
        "  # Show the result\n",
        "  formatted_data_df.show()\n",
        "\n",
        "  return formatted_data_df\n"
      ],
      "metadata": {
        "id": "F3LV3dd1o4Ph"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  # Assuming we working with the specified columns\n",
        "  input_columns = [\"Age\", \"Total_Purchase\", \"Account_Manager\", \"Years\", \"Num_Sites\"]\n",
        "  formatted_data_df = format_df_by_cols(input_columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-z5FjJy3scoQ",
        "outputId": "e56a1bdd-e750-4679-b275-d59aa9156b33"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+----+--------------+---------------+-----+---------+-------------------+--------------------+--------------------+-----+--------------------+\n",
            "|              Names| Age|Total_Purchase|Account_Manager|Years|Num_Sites|       Onboard_date|            Location|             Company|Churn|            features|\n",
            "+-------------------+----+--------------+---------------+-----+---------+-------------------+--------------------+--------------------+-----+--------------------+\n",
            "|   Cameron Williams|42.0|       11066.8|              0| 7.22|      8.0|2013-08-30 07:00:40|10265 Elizabeth M...|          Harvey LLC|    1|[42.0,11066.8,0.0...|\n",
            "|      Kevin Mueller|41.0|      11916.22|              0|  6.5|     11.0|2013-08-13 00:38:46|6157 Frank Garden...|          Wilson PLC|    1|[41.0,11916.22,0....|\n",
            "|        Eric Lozano|38.0|      12884.75|              0| 6.67|     12.0|2016-06-29 06:20:07|1331 Keith Court ...|Miller, Johnson a...|    1|[38.0,12884.75,0....|\n",
            "|      Phillip White|42.0|       8010.76|              0| 6.71|     10.0|2014-04-22 12:43:12|13120 Daniel Moun...|           Smith Inc|    1|[42.0,8010.76,0.0...|\n",
            "|     Cynthia Norton|37.0|       9191.58|              0| 5.56|      9.0|2016-01-19 15:31:15|765 Tricia Row Ka...|          Love-Jones|    1|[37.0,9191.58,0.0...|\n",
            "|   Jessica Williams|48.0|      10356.02|              0| 5.12|      8.0|2009-03-03 23:13:37|6187 Olson Mounta...|        Kelly-Warren|    1|[48.0,10356.02,0....|\n",
            "|        Eric Butler|44.0|      11331.58|              1| 5.23|     11.0|2016-12-05 03:35:43|4846 Savannah Roa...|   Reynolds-Sheppard|    1|[44.0,11331.58,1....|\n",
            "|      Zachary Walsh|32.0|       9885.12|              1| 6.92|      9.0|2006-03-09 14:50:20|25271 Roy Express...|          Singh-Cole|    1|[32.0,9885.12,1.0...|\n",
            "|        Ashlee Carr|43.0|       14062.6|              1| 5.46|     11.0|2011-09-29 05:47:23|3725 Caroline Str...|           Lopez PLC|    1|[43.0,14062.6,1.0...|\n",
            "|     Jennifer Lynch|40.0|       8066.94|              1| 7.11|     11.0|2006-03-28 15:42:45|363 Sandra Lodge ...|       Reed-Martinez|    1|[40.0,8066.94,1.0...|\n",
            "|       Paula Harris|30.0|      11575.37|              1| 5.22|      8.0|2016-11-13 13:13:01|Unit 8120 Box 916...|Briggs, Lamb and ...|    1|[30.0,11575.37,1....|\n",
            "|     Bruce Phillips|45.0|       8771.02|              1| 6.64|     11.0|2015-05-28 12:14:03|Unit 1895 Box 094...|    Figueroa-Maynard|    1|[45.0,8771.02,1.0...|\n",
            "|       Craig Garner|45.0|       8988.67|              1| 4.84|     11.0|2011-02-16 08:10:47|897 Kelley Overpa...|     Abbott-Thompson|    1|[45.0,8988.67,1.0...|\n",
            "|       Nicole Olson|40.0|       8283.32|              1|  5.1|     13.0|2012-11-22 05:35:03|11488 Weaver Cape...|Smith, Kim and Ma...|    1|[40.0,8283.32,1.0...|\n",
            "|     Harold Griffin|41.0|       6569.87|              1|  4.3|     11.0|2015-03-28 02:13:44|1774 Peter Row Ap...|Snyder, Lee and M...|    1|[41.0,6569.87,1.0...|\n",
            "|       James Wright|38.0|      10494.82|              1| 6.81|     12.0|2015-07-22 08:38:40|45408 David Path ...|      Sanders-Pierce|    1|[38.0,10494.82,1....|\n",
            "|      Doris Wilkins|45.0|       8213.41|              1| 7.35|     11.0|2006-09-03 06:13:55|28216 Wright Moun...|Andrews, Adams an...|    1|[45.0,8213.41,1.0...|\n",
            "|Katherine Carpenter|43.0|      11226.88|              0| 8.08|     12.0|2006-10-22 04:42:38|Unit 4948 Box 481...|Morgan, Phillips ...|    1|[43.0,11226.88,0....|\n",
            "|     Lindsay Martin|53.0|       5515.09|              0| 6.85|      8.0|2015-10-07 00:27:10|69203 Crosby Divi...|      Villanueva LLC|    1|[53.0,5515.09,0.0...|\n",
            "|        Kathy Curry|46.0|        8046.4|              1| 5.69|      8.0|2014-11-06 23:47:14|9569 Caldwell Cre...|Berry, Orr and Ca...|    1|[46.0,8046.4,1.0,...|\n",
            "+-------------------+----+--------------+---------------+-----+---------+-------------------+--------------------+--------------------+-----+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.Split the data into train/test data, and then fit train data to the logistic regression model."
      ],
      "metadata": {
        "id": "70PvPQV9udNH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.tuning import TrainValidationSplit, ParamGridBuilder\n",
        "\n",
        "def get_train_model(df,test_size=0.2):\n",
        "\n",
        "  train_data, test_data = df.randomSplit([1-test_size, test_size], seed=42)\n",
        "\n",
        "  return train_data, test_data\n"
      ],
      "metadata": {
        "id": "XOJTgjHmuiss"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, test_data  = get_train_model(formatted_data_df, 0.2)"
      ],
      "metadata": {
        "id": "b6L5BxiJvRpR"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the logistic regression model\n",
        "logistic_regression  = LogisticRegression(labelCol=\"Churn\", featuresCol=\"features\")\n",
        "\n",
        "# Set up parameter grid for hyperparameter tuning (if needed)\n",
        "param_grid = (ParamGridBuilder()\n",
        "              .addGrid(logistic_regression.regParam, [0.01, 0.1, 0.5])  # Regularization parameter\n",
        "              .addGrid(logistic_regression.elasticNetParam, [0.0, 0.5, 1.0])  # Elastic Net mixing parameter\n",
        "              .build())\n",
        "\n",
        "# Set up the TrainValidationSplit for hyperparameter tuning (if needed)\n",
        "validator = TrainValidationSplit(estimator=logistic_regression , estimatorParamMaps=param_grid, evaluator=BinaryClassificationEvaluator(), trainRatio=0.7)\n",
        "\n",
        "# Fit the logistic regression model to the train data\n",
        "logistic_model = logistic_regression.fit(train_data)"
      ],
      "metadata": {
        "id": "LrN9xUlFxQEe"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4.Evaluate the results and compute the AUC."
      ],
      "metadata": {
        "id": "P3k1ogQAvYnd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on the test data\n",
        "predictions = logistic_model.transform(test_data)\n",
        "\n",
        "# Evaluate the model using AUC\n",
        "evaluator = BinaryClassificationEvaluator(labelCol=\"Churn\", metricName=\"areaUnderROC\")\n",
        "area_under_curve = evaluator.evaluate(predictions)\n",
        "\n",
        "print(\"Area Under ROC Curve: {:.2f}\".format(area_under_curve))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "noJqgHQfrkld",
        "outputId": "7e134827-0017-4572-ea1b-0e661d2fd5f6"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Area Under ROC Curve: 0.88\n"
          ]
        }
      ]
    }
  ]
}